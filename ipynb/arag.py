# -*- coding: utf-8 -*-
"""ARAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gaviIiu-fb7BnDbZzmkLv-ehxVww5wy-
"""

!pip install accelerate bitsandbytes transformers peft datasets trl

!pip install -U sentence-transformers transformers

import pandas as pd
df = pd.read_csv('parsing.csv')
documents = df.apply(lambda row: f"Q: {row['Question']}\nA: {row['Answer']}", axis=1).tolist()

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2", cache_folder="downloader")
embeddings = model.encode(documents, convert_to_tensor=True)

!pip install faiss-cpu

import faiss
import numpy as np

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings.cpu()))
# Save FAISS index
faiss.write_index(index, "index.faiss")

# Save your documents
with open("context_docs.txt", "w") as f:
    f.write("\n---\n".join(documents))

# Later, reload like this:
index = faiss.read_index("index.faiss")
with open("context_docs.txt") as f:
    documents = f.read().split("\n---\n")

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from transformers import BitsAndBytesConfig
model_id = "Norquinal/Mistral-7B-claude-chat"
tokenizer = AutoTokenizer.from_pretrained("Norquinal/Mistral-7B-claude-chat", cache_dir="downloader")
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=torch.float16, offload_folder="/offload", cache_dir="downloader")

def generate_answer(query, index, documents, embed_model, lm_model, tokenizer, max_tokens=300):
    import numpy as np
    import time

    # Step 1: Embed the query
    query_embedding = embed_model.encode(query)

    # Step 2: Retrieve relevant chunks
    D, I = index.search(np.array([query_embedding]), k=5)

    # Step 3: Clean retrieved chunks to strip Q: and A:
    def strip_qa_noise(text):
        lines = text.split("\n")
        return "\n".join(line for line in lines if not line.strip().startswith(("Q:", "A:", "Question:", "Answer:")))

    retrieved = [strip_qa_noise(documents[i]) for i in I[0]]
    context = "\n".join(retrieved)

    # Step 4: Format prompt with clear instructions
    prompt = f"""You are a helpful technical assistant.

Use only the following context to answer the current question. Do not repeat unrelated prior questions.

Context:
{context}

Question: {query}
Answer:"""

    # Step 5: Tokenize and generate
    device = next(lm_model.parameters()).device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    start = time.time()
    output = lm_model.generate(**inputs, max_new_tokens=max_tokens)


    return tokenizer.decode(output[0], skip_special_tokens=True)



generate_answer(
    query="What should be done if the edge/cluster fails to come up after a management interface toggle",
    index=index,
    documents=documents,
    embed_model=SentenceTransformer("all-MiniLM-L6-v2"),
    lm_model=model,
    tokenizer=tokenizer
)

!cp index.faiss /content/drive/MyDrive/rag/index.faiss

!cp context_docs.txt /content/drive/MyDrive/rag/context_docs.txt

!cp -r downloader /content/drive/MyDrive/rag/downloader