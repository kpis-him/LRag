Create your own RAG pipeline and model:

1) Install required libraries(hugginface hub, lanchain, sentence_transformers, faiss-cpu etc)

2) Get your knowledgebase, compile it into a csv or txt file with Question answer format, look at parsing.csv for more details about how the file should look

3) Choose a sentence transformer, I chose MiniLM

4) FAISS(Facebook AI Similarity Search) is used with RAGs to be able to generate vector databases that are used to choose the best QnA pair, a pkl file is also generated to help the faiss go through smoothly by storing serialized Python objects like dictionaries, lists etc

5) Pull the original model from huggingface, i chose Norquinal/Mistral-Claude-Chat

6) download the index.faiss, pkl, and your knowledgebase and use llama.cpp

7) llama.cpp has a convert_to_gguf_hf.py file

8)use you faiss and pkl files to run with create_faiss_index.py

9)Run your model with run_rag.py!
