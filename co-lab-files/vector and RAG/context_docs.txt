Q: What should be done if the edge/cluster fails to come up after a management interface toggle (ONECELL5G-1068)?
A: 1. Power off the Standby node. 2. Reboot the Active node. 3. Once the Active node is up, power on the Standby node.
---
Q: How can "Imagepullbackoff" errors be resolved for rbd-provisioner pods (ONECELL5G-1546)?
A: This can occur if the rbd-provisioner image is not replicated on the edge local repository or if the standby node cannot resolve "registry.local". 1. Pull the image using `ctr images pull <central-floating-ip>:30003/starlingx/quay.io/external_storage/rbd-provisioner:v2.1.1-k8s1.11`. 2. Tag the image using `ctr images tag <central-floating-ip>:30003/starlingx/quay.io/external_storage/rbd-provisioner:v2.1.1-k8s1.11 registry.local:9001/quay.io/external_storage/rbd-provisioner:v2.1.1-k8s1.11`. 3. Push the image using `ctr images push registry.local:9001/quay.io/external_storage/rbd-provisioner:v2.1.1-k8s1.11 --skip-verify -u admin`. Use the password for the Active controller. 4. Run `netstat -ant | grep 9001` to get the OAM IP address of a registry process. 5. Add this OAM IP address to the `/etc/hosts` file of the node where the image pull is failing.
---
Q: Why is a sysadmin password update not effective system-wide when updated via NODE-IP, and how should it be updated?
A: The Floating OAM IP is only associated with the active controller and it is recommended to access the cluster only over OAM floating IP. On expiry of sysadmin password, the password needs to be updated via the Active controller only, and the controller should be Online and in an available state. The password will auto sync after some time.
---
Q: What causes the "Failed to pull images from registry.local" error and how can it be resolved?
A: This error is due to missing executable permissions on the "commscope_patch.sh" file. Ensure executable permission is assigned to this file.
---
Q: How can RabbitMQ connection failures be troubleshooted when reconnecting to AMQP server on 10.222.35.1:5672 results in `ECONNREFUSED`?
A: This indicates the AMQP server is unreachable. The system will try again in 30 seconds.
---
Q: What can cause controller-0 to not become active after bootstrap and reboot, and lead to the `/etc/platform/openrc` command failing?
A: This can happen if the initial configuration is not completed. You might see messages in the log indicating issues with platform filesystem backup.
---
Q: What indicates an incomplete initial configuration that prevents the controller from coming up?
A: Log messages like `Error: ... /Stage[main]/Platform::Filesystem::Backup/... change from 25G to 50G failed: Execution of '/usr/sbin/lvextend -L 52428800k "/dev/cgts-vg/backup-lv"' returned 5: Insufficient free space: 800 extents needed, but only 37 available` or `Notice: ... /Stage[main]/Platform::Filesystem::Backup/... Exec[wipe start of device backup-lv]: Dependency Logical_volume[backup-lv] has failures: true` indicate incomplete configuration.
---
Q: What should be checked if a node continuously restarts after a flash install?
A: Check the console logs for disk error messages.
---
Q: Why might Nginx not be able to forward VES events?
A: Curl or Nginx might not work without binding to an IP address, or there might be a kernel version difference. The `proxy_bind` option needs to be used in Nginx, requiring code changes.
---
Q: What happens when the `/var/lib/docker` partition becomes full?
A: Pods are evicted when the `/var/lib/docker` partition becomes full.
---
Q: How can a full `/var/lib/docker` partition be resolved?
A: Increase the file system disk to 50G for docker through the StarlingX central cloud UI.
---
Q: Why is the CMPv2 application crashing?
A: The Java application is reporting no available cores.
---
Q: How can the CMPv2 application crashing due to no available cores be fixed?
A: The Java Application needs to be run with the `-Dio.grpc.netty.shaded.io.netty.availableProcessors=1` flag, which requires code changes.
---
Q: What are common reasons for bootstrap failure on central cloud controller 0 due to image pull errors?
A: If the setup depends on an external harbor that requires a certificate for authentication, the correct certificate might not have been added to the `localhost.yaml` file. Look for "FAILED - RETRYING: Log in k8s, gcr, quay, docker registries if credentials exist" in `ansible.log`. If using a jfrog repo in `localhost.yaml` for image pull during bootstrap, username and password under `defaults` of `docker_registries` should *not* be present in `localhost.yaml`.
---
Q: How can image pull errors during bootstrap be resolved when using an external harbor?
A: Validate the certificate for the external repository and mention it in the `localhost.yaml` file, for example: `ssl_ca_cert: /home/sysadmin/sysadmin/ca.crt`.
---
Q: What causes "Error Populating transactions after 10 retries: failure: Packages/perf-3.10.0-1160.15.2.e17.4.tis.x86_64.rpm from anaconda: [Errorno 256] No more mirrors to try" during Windriver installation?
A: This can happen if the CD/DVD was selected in boot options in BIOS on a Super Micro Server.
---
Q: How can the "No more mirrors to try" error during Windriver installation be resolved?
A: 1. Select "UEFI Hard Disk" in boot options from BIOS. 2. Mount the ISO image and reboot the server. 3. Select "UEFI: ATEN Virtual CDROM YSOJ" option by pressing F11.
---
Q: What causes the server to reboot back to the installation process after a successful Windriver installation?
A: This happens if the BIOS option is not set correctly.
---
Q: How can the server rebooting back to the installation process after a successful Windriver installation be resolved?
A: 1. Change the BIOS option to "UEFI Hard disk, USB UEFI Hard disk, UEFI: ATEN Virtual CDROM YSOJ, UEFI CD/DVD". 2. Plug out the ISO image in virtual media.
---
Q: How can the installation error "There was already a backup- You requested a partition from 10.5GB to 10.8GB....." be resolved?
A: 1. Get the console. 2. Go to the shell by "Ctrl+o". 3. Run command: "parted". 4. List the partition by running: "print". 5. Delete all listed partitions by running: "rm #index". 6. Exit parted: "quit". 7. Retry the installation.
---
Q: When is this installation error observed?
A: This issue is observed when the user updates the boot mode and tries to reinstall the node.
---
Q: How can Jumbo MTU be persisted or modified for a data interface?
A: This procedure can be followed if the setup was brought up using an older guide. 1. Ensure Controller-1 is the standby node. If not, perform a switch-over using the GUI. 2. CLI Login to the edge's active node using the floating OAM IP. 3. Lock Controller-1 using Web-GUI or the command `system host-lock controller-1`. 4. Set `DATAOQIF=<DATA_INTERFACE>` (e.g., `enp101s0fO`), `NODE=controller-1`, `PHYSNETO='physnet@'`, `SPL=/tmp/tmp-system-port-list`, `SPIL=/tmp/tmp-system-host-if-list`. 5. Run `system host-port-list ${NODE} --nowrap > ${SPL}`. 6. Run `system host-if-list -a ${NODE} --nowrap > ${SPIL}`. 7. Get PCI address: `DATAOPCIADDR=$(cat $SPL | grep $DATAOIF | awk â€˜{print "$8}')`. 8. Get port UUID: `DATAOPORTUUID=$(cat $SPL | grep ${DATAOPCIADDR} | awk '{print "$2}')`. 9. Get port name: `DATAOPORTNAME=$(cat $SPL | grep ${DATAOPCIADDR} | awk '{print "$4}')`. 10. Get interface UUID: `DATAOIFUUID=$(cat $SPIL | awk -v DATAOPORTNAME=$DATAOPORTNAME '($12 ~ DATAOPORTNAME) {print "$2}')`. 11. Modify the interface: `system host-if-modify controller-1 -n sriovO -m 9216 --vf-driver=netdevice $DATAOIFUUID`. 12. Unlock Controller-1 using GUI or the command `system host-unlock controller-1`.
---
Q: Why might PTP interface configuration not persist across reboots?
A: This happens if the PTP interface was not added to the StarlingX configuration. The PTP interface must be connected, and "eno2" is the hardcoded interface for PTP.
---
Q: How can PTP interface configurations be persisted?
A: 1. Run `sudo ifconfig eno2 up`. 2. Verify the `eno2` link status by running `ethtool eno2`. 3. Ensure Controller-1 is the standby node. If not, perform a switch-over using the GUI. 4. CLI Login to the edge's active node using the floating OAM IP. 5. Lock Controller-1 using Web-GUI or the command `system host-lock controller-1`. 6. Set `PTP_INTERFACE_IP=<PTP_INTERFACE_IP>` (e.g., `192.190.20.232`), `PTP_INTERFACE_PREFIX=<PTP_INTERFACE_PREFIX>` (e.g., `124`), `PTP_IFC=eno2`. 7. Run `system host-if-modify -n ${PTP_IFC} -c platform controller-1 eno2 --ipv4-mode=static`. 8. Run `system host-addr-add controller-1 ${PTP_IFC} ${PTP_INTERFACE_IP} ${PTP_INTERFACE_PREFIX}`. 9. If updating the IP address, remove the old IP first using `system host-addr-list controller-1` to list IPs and `system host-addr-delete <UUID_HOST_ADD>` to delete the old IP.
---
Q: What does it mean if `/proc/cmdline` does not display `intel_pstate`?
A: This generally happens when the user has not set the boot mode as "UEFI".
---
Q: How can it be validated if the system is in UEFI mode?
A: Check if `/sys/firmware/efi/config_table` exists. If it doesn't, the system is likely in BIOS mode, not UEFI.
---
Q: How to set boot mode to UEFI?
A: 1. Reboot the system and go to SETUP Mode (using DEL). 2. Go to the BOOT section. 3. Update the "Boot mode" to "UEFI". 4. Save the configuration and perform a re-install on the node.
---
Q: Why is the DNS IP address mentioned in the edge's `bootstrap.yml` file overwritten by other IPs?
A: This happens because DNS is a shared configuration managed by the System Controller and synchronized across all edges. Once the edge is managed, the DNS list will be overwritten by the central cloud.
---
Q: How can the DNS list on the edge be updated?
A: 1. First, ensure the central cloud has the desired DNS list to prevent this situation. 2. Alternatively, update the edge's system configuration using the web-GUI. 3. Steps to update edge's DNS via GUI: Login to Horizon GUI and navigate to the desired edge (subclouds -> [edgeX] -> Host details). Navigate to the system configuration page (Admin -> Platform -> System Configuration in the left-hand pane). Select the DNS tab. Click "Edit DNS" and add or edit the IP addresses, then click Save.
---
Q: Why does the k8s cluster not come up on controller-0 after unlocking it, resulting in "The connection to the server 192.168.206.1:6443 was refused"?
A: It is recommended to go ahead with the re-install of the controller node.
---
Q: Why does a host go to a failed state?
A: This is generally seen on configuration failure when there is a user mistake in configuring the system. If the host experiences a configuration failure, it attempts auto-recovery. If it cannot auto-recover (after a threshold is reached), it transitions to a Failed state. This is expected behavior.
---
Q: How can a host in a failed state be recovered?
A: Lock and unlock the node. The host status field and alarms will indicate this recovery method.
---
Q: What should be done if the "host-lock" operation cannot be performed from the Horizon-GUI?
A: Use the system CLI. 1. SSH to the edge using its OAM-floating-IP address as a sysadmin user. 2. Acquire keystone access by running `source /etc/platform/openrc`. 3. Lock the faulty host: `system host-lock <host-name>`. 4. Unlock the faulty host: `system host-unlock <host-name>`.
---
Q: How should a license be updated before its expiry?
A: The new license needs to be applied to each cloud-system (central or edge).
---
Q: What are the steps to apply a new license?
A: 1. Upload the `<License File>` to the system using its OAM_FLOATING IP. 2. Acquire keystone access: `source /etc/platform/openrc`. 3. Apply the license: `system license-install <License File>`. The expected output is "Success: new license installed".
---
Q: What are the consequences of updating a license after its expiry?
A: Manual recovery of the node may be required if the license is upgraded after expiry. It is recommended to update licenses before they expire.
---
Q: How can a node be recovered if the license is upgraded after expiry, assuming controller-0 was an active node?
A: 1. Power-off the standby node (controller-1). 2. Upload the `<License File>` to the Active node (controller-0) using its OAM IP. 3. CLI login to controller-0 using its OAM_IP and run `sudo /usr/sbin/license-install <License File>`. 4. Wait for the Cloud Platform to auto-recover (up to 10 min). If controller-0 doesn't auto-recover, reboot it and wait for it to come up as Active. If it auto-recovers, the user should be able to acquire keystone access. 5. Acquire keystone access: `source /etc/platform/openrc`. 6. Apply the system-wide license: `system license-install <License File>`. 7. Power-on controller-1.
---
Q: Why is it necessary to update default file system sizes?
A: Some default system sizes are not sufficient and can cause problems in the long run. For example, core file size and current log size might not be enough, and the default docker size might be too small if there are many images.
---
Q: How can the file system size be updated in Central Cloud (CC) or Edge Cloud (EC)?
A: The file-system size needs to be updated for both nodes. 1. As a sysadmin user, login to the Active node using the EC/CC OAM floating IP address and acquire keystone access: `source /etc/platform/openrc`. 2. Update the file-system size for controller-0. Create an additional partition of size 260G in controller-0 and add this partition as a persistence volume under the `cgts-vg` volume group. Set `NODE=controller-0`. Run `system host-disk-partition-add -t lvm_phys_vol ${NODE} /dev/sda 260`. Partition creation may take time; run `system host-disk-partition-list ${NODE}` to ensure `/dev/sda6` status is "Ready" before proceeding. Run `system host-pv-add ${NODE} cgts-vg $(system host-disk-partition-list ${NODE} | grep "/dev/sda" | awk -F '"' '{print "$2}')`. Ensure `system host-pv-list ${NODE}` shows `/dev/sda6` as provisioned. Increase the backup, docker, kubelet, and scratch filesystem sizes: `system host-fs-modify ${NODE} backup=50` `system host-fs-modify ${NODE} docker=60` `system host-fs-modify ${NODE} kubelet=80` `system host-fs-modify ${NODE} scratch=66` Increase log filesystem size: Login to controller-0 and run `sudo lvextend -L+50G /dev/cgts-vg/log-lv`.